{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa13e3f7-b2c9-4a0c-ba7c-289acee30f9c",
   "metadata": {},
   "source": [
    "## Parsing FAIR evaluation metrics\n",
    "\n",
    "This notebook was used to develop the logic of the FAIR metrics profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b88d8ec-6220-4919-8e67-63dd27515bb4",
   "metadata": {},
   "source": [
    "---\n",
    "### Sample data files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f38952-9ee0-4610-abb6-e9f8c1ad56de",
   "metadata": {},
   "source": [
    "|DOI | Example | File Name |\n",
    "|---|---|---|\n",
    "| https://doi.org/10.3233/FAIA200871 | Example 1: Publication metadata | fair_metrics_1_publication.json |\n",
    "| https://doi.org/10.1145/3184558.3191543 | Example 2: Publication metadata  | fair_metrics_2_publication.json |\n",
    "|https://doi.org/10.5281/zenodo.8148685| Example 3: Software metadata | fair_metrics_3_software.json |\n",
    "|https://doi.org/10.34894/Q80QUE| Example 4: Dataset metadata | fair_metrics_4_dataset.json |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4a21578-56d3-4a74-a67b-afc1ccd699be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of file names\n",
    "file_data = [\n",
    "    {\n",
    "        \"DOI\": \"https://doi.org/10.3233/FAIA200871\",\n",
    "        \"File Name\": \"fair_metrics_1_publication.json\"\n",
    "    },\n",
    "    {\n",
    "        \"DOI\": \"https://doi.org/10.1145/3184558.3191543\",\n",
    "        \"File Name\": \"fair_metrics_2_publication.json\"\n",
    "    },\n",
    "    {\n",
    "        \"DOI\": \"https://doi.org/10.5281/zenodo.8148685\",\n",
    "        \"File Name\": \"fair_metrics_3_software.json\"\n",
    "    },\n",
    "    {\n",
    "        \"DOI\": \"https://doi.org/10.34894/Q80QUE\",\n",
    "        \"File Name\": \"fair_metrics_4_dataset.json\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cef368f-eae6-4fae-967f-e119f65c19b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Metadata FAIR metrics\n",
    "attributes = [\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/f1-metadata-identifier-persistent\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/f1-metadata-identifier-unique\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/a1-metadata-authorization\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/a1-metadata-protocol\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/f2-structured-metadata\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/f1-data-identifier-persistent\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/f3-metadata-identifier-in-metadata\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/i3-metadata-contains-outward-links\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/r1-includes-license\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/f2-grounded-metadata\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/a1-data-protocol\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/a1-data-authorization\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/a2-metadata-persistent\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/f3-data-identifier-in-metadata\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/i1-data-knowledge-representation-structured\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/i1-metadata-knowledge-representation-structured\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/f4-searchable\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/i1-data-knowledge-representation-semantic\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/i2-fair-vocabularies-known\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/i1-metadata-knowledge-representation-semantic\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/r1-includes-standard-license\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/i2-fair-vocabularies-resolve\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88708591-7e4c-4fb2-ac25-a277e321435f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/hh5hqvzj7szdjc24hlsbn00h0000gn/T/ipykernel_62148/1830059240.py:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_data = all_data.append(df)\n",
      "/var/folders/s8/hh5hqvzj7szdjc24hlsbn00h0000gn/T/ipykernel_62148/1830059240.py:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_data = all_data.append(df)\n",
      "/var/folders/s8/hh5hqvzj7szdjc24hlsbn00h0000gn/T/ipykernel_62148/1830059240.py:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_data = all_data.append(df)\n",
      "/var/folders/s8/hh5hqvzj7szdjc24hlsbn00h0000gn/T/ipykernel_62148/1830059240.py:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_data = all_data.append(df)\n",
      "/var/folders/s8/hh5hqvzj7szdjc24hlsbn00h0000gn/T/ipykernel_62148/1830059240.py:47: FutureWarning: As the xlwt package is no longer maintained, the xlwt engine will be removed in a future version of pandas. This is the only engine in pandas that supports writing in the xls format. Install openpyxl and write to an xlsx file instead. You can set the option io.excel.xls.writer to 'xlwt' to silence this warning. While this option is deprecated and will also raise a warning, it can be globally set and the warning suppressed.\n",
      "  all_data.to_excel(\"FAIR_metrics_report.xls\", index=False)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Initialize an empty dataframe\n",
    "all_data = pd.DataFrame()\n",
    "\n",
    "# Iterate through each file\n",
    "for file_info in file_data:\n",
    "    with open(f\"../data_sample/{file_info['File Name']}\", \"r\") as file:\n",
    "        data_sample = json.load(file)\n",
    "    \n",
    "    rows = []\n",
    "\n",
    "    for attribute in attributes:\n",
    "        metric_results = data_sample[\"contains\"].get(attribute, [])\n",
    "        if not metric_results:\n",
    "            continue\n",
    "\n",
    "        metric_result = metric_results[0]\n",
    "        value = metric_result.get(\"http://semanticscience.org/resource/SIO_000300\", [{}])[0].get(\"@value\", None)\n",
    "        comment = metric_result.get(\"http://schema.org/comment\", [{}])[0].get(\"@value\", None)\n",
    "\n",
    "        # Extracting the last prompt from the comment\n",
    "        if comment:\n",
    "            matches = re.findall(r\"(INFO|SUCCESS|FAILURE): \\[\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\] .+\", comment)\n",
    "            if matches:\n",
    "                comment = matches[-1]  # Taking the last match\n",
    "\n",
    "        # Build each row\n",
    "        rows.append({\n",
    "            \"_id\": data_sample[\"_id\"],\n",
    "            \"subject\": data_sample[\"subject\"],\n",
    "            \"created_at\": data_sample[\"created_at\"],\n",
    "            \"name\": data_sample[\"name\"],\n",
    "            \"fair_metric\": attribute.split(\"/\")[-1],  # Get the last part of the URL for a cleaner metric name\n",
    "            \"value\": value,\n",
    "            \"comment\": comment,\n",
    "            \"DOI\": file_info[\"DOI\"],  # Adding DOI to the dataframe\n",
    "        })\n",
    "    \n",
    "    # Convert rows to DataFrame and append to the overall dataframe\n",
    "    df = pd.DataFrame(rows)\n",
    "    all_data = all_data.append(df)\n",
    "\n",
    "# Export the combined dataframe to an excel file\n",
    "all_data.to_excel(\"FAIR_metrics_report.xls\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c95c814-0636-44b8-ab73-17f0e8c7d6f8",
   "metadata": {},
   "source": [
    "---\n",
    "### Data directly from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f400db-9f6c-4cc5-9d15-e31d68273e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "\n",
    "def fetch_fair_evaluation(doi_url):\n",
    "    \"\"\"\n",
    "    Fetches the FAIR evaluation for a given DOI URL using the FAIR API.\n",
    "    \n",
    "    Args:\n",
    "    - doi_url (str): The DOI URL to be evaluated.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: The JSON response from the FAIR API.\n",
    "    \"\"\"\n",
    "    # API endpoint\n",
    "    api_url = \"https://api.fair-enough.semanticscience.org/evaluations\"\n",
    "    \n",
    "    # Headers for the request\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    # Data payload for the request\n",
    "    data = {\n",
    "        \"subject\": doi_url,\n",
    "        \"collection\": \"fair-enough-metadata\"  # Adjusted based on the provided documentation\n",
    "    }\n",
    "    \n",
    "    # Making the POST request to the FAIR API\n",
    "    response = requests.post(api_url, headers=headers, json=data)\n",
    "    \n",
    "    # Check for valid response\n",
    "    if response.status_code == 201:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error {response.status_code}: {response.text}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d4bcde-75a0-400d-92f8-4b676359dd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Metadata FAIR metrics\n",
    "attributes = [\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/f1-metadata-identifier-persistent\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/f1-metadata-identifier-unique\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/a1-metadata-authorization\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/a1-metadata-protocol\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/f2-structured-metadata\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/f1-data-identifier-persistent\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/f3-metadata-identifier-in-metadata\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/i3-metadata-contains-outward-links\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/r1-includes-license\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/f2-grounded-metadata\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/a1-data-protocol\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/a1-data-authorization\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/a2-metadata-persistent\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/f3-data-identifier-in-metadata\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/i1-data-knowledge-representation-structured\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/i1-metadata-knowledge-representation-structured\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/f4-searchable\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/i1-data-knowledge-representation-semantic\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/i2-fair-vocabularies-known\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/i1-metadata-knowledge-representation-semantic\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/r1-includes-standard-license\",\n",
    "    \"https://w3id.org/fair-enough/metrics/tests/i2-fair-vocabularies-resolve\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec335c1-603c-4f70-8ce0-d046833f06ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of DOIs\n",
    "dois = [\n",
    "    \"https://doi.org/10.3233/FAIA200871\",\n",
    "    \"https://doi.org/10.1145/3184558.3191543\",\n",
    "    \"https://doi.org/10.5281/zenodo.8148685\",\n",
    "    \"https://doi.org/10.34894/Q80QUE\"\n",
    "]\n",
    "\n",
    "# Initialize an empty dataframe\n",
    "all_data = pd.DataFrame()\n",
    "\n",
    "# Iterate through each DOI\n",
    "for doi in dois:\n",
    "    data_sample = fetch_fair_evaluation(doi)\n",
    "    if not data_sample:\n",
    "        continue  # Skip to the next iteration if no data is retrieved for this DOI\n",
    "    \n",
    "    rows = []\n",
    "\n",
    "    for attribute in attributes:\n",
    "        metric_results = data_sample[\"contains\"].get(attribute, [])\n",
    "        if not metric_results:\n",
    "            continue\n",
    "\n",
    "        metric_result = metric_results[0]\n",
    "        value = metric_result.get(\"http://semanticscience.org/resource/SIO_000300\", [{}])[0].get(\"@value\", None)\n",
    "        comment = metric_result.get(\"http://schema.org/comment\", [{}])[0].get(\"@value\", None)\n",
    "\n",
    "        # Extracting the last prompt from the comment\n",
    "        if comment:\n",
    "            matches = re.findall(r\"(INFO|SUCCESS|FAILURE): \\[\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\] .+\", comment)\n",
    "            if matches:\n",
    "                comment = matches[-1]  # Taking the last match\n",
    "\n",
    "        # Build each row\n",
    "        rows.append({\n",
    "            \"_id\": data_sample[\"_id\"],\n",
    "            \"subject\": data_sample[\"subject\"],\n",
    "            \"created_at\": data_sample[\"created_at\"],\n",
    "            \"name\": data_sample[\"name\"],\n",
    "            \"fair_metric\": attribute.split(\"/\")[-1],  # Get the last part of the URL for a cleaner metric name\n",
    "            \"value\": value,\n",
    "            \"comment\": comment,\n",
    "            \"DOI\": doi  # Adding DOI to the dataframe\n",
    "        })\n",
    "    \n",
    "    # Convert rows to DataFrame and append to the overall dataframe\n",
    "    df = pd.DataFrame(rows)\n",
    "    all_data = all_data.append(df)\n",
    "\n",
    "# Export the combined dataframe to an excel file\n",
    "all_data.to_excel(\"FAIR_metrics_report_from_API.xls\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876e858b-6500-424a-ab1b-9f373825aa61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
